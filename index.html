<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Pranay Junare</title> <meta name="author" content="Pranay Junare"> <meta name="description" content="Personal webite for Pranay Junare. "> <meta name="keywords" content="pranay-junare, robotics, computer-vision, jekyll, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://pranay-junare.github.io/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/experience/">Experience</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <div class="nav-item CV"> <a class="nav-link" href="/assets/pdf/PranayJunare_Resume.pdf"> CV </a> </div> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Pranay</span> Junare </h1> <p class="desc">Computer Vision | Deep Learning | Machine Learning</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/personal_img/Pranay_pic-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/personal_img/Pranay_pic-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/personal_img/Pranay_pic-1400.webp"></source> <img src="/assets/img/personal_img/Pranay_pic.jpeg?5db14f17fdfbd21902f224454b9045f9" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="personal_img/Pranay_pic.jpeg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="more-info"> <p>junar002@umn.edu</p> <p><a href="mailto:junar002@umn.edu">Email</a> | <a href="https://scholar.google.com/citations?user=38XpwpkAAAAJ&amp;hl" rel="external nofollow noopener" target="_blank">Google Scholar</a> </p> <br> <p><a href="/assets/pdf/PranayJunare_Resume.pdf">CV</a> | <a href="https://github.com/pranay-junare" rel="external nofollow noopener" target="_blank">Github</a> | </p> <p><a href="https://www.linkedin.com/in/pranay-junare" rel="external nofollow noopener" target="_blank">LinkedIn</a></p> <br> <p>Minneapolis, MN, USA</p> </div> </div> <div class="clearfix"> <p>Hello! I’m a MS. in Robotics student at University of Minnesota (UMN)-Twin Cities, an Ex-Software Developer at ION Trading, past Research Intern at NTU-Singapore, and a Research Scholar at MITACS-Canada.</p> <p>I am fundamentally passionate about understanding how human perception &amp; manipulation work and how we can build autonomous systems that have tremendous positive social impact. In pursuit of this passion, I aim to build autonomous perception systems that are at par with the inherent human cognitive capabilities. My research interests broadly lies in <code class="language-plaintext highlighter-rouge">Vision-based Autonomous Systems</code> especially in the domains of <code class="language-plaintext highlighter-rouge">Robot Learning, Perception, and 3D-Computer Vision</code>.</p> <p>During my undergraduate days, I was a Team Member of University’s Robotics Laboratory advised by <a href="https://www.coep.org.in/mycoep/ssomechcoepacin" rel="external nofollow noopener" target="_blank">Dr. Shantipal Ohol</a> and have also interned with 2 startups <a href="https://www.exa-mobility.com/" rel="external nofollow noopener" target="_blank">Exa Mobility</a> and <a href="https://www.binaryrobotics.in/" rel="external nofollow noopener" target="_blank">Binary Robotics</a>.</p> <p>In addition, I was a former Summer Research Intern at <a href="https://www.ntu.edu.sg/" rel="external nofollow noopener" target="_blank">NTU-Singapore</a> under <a href="https://dr.ntu.edu.sg/cris/rp/rp00965" rel="external nofollow noopener" target="_blank">Dr. Xie Ming</a> and was funded by the <code class="language-plaintext highlighter-rouge">NTU-India Connect Research Fellowship</code>. I was also fortunate enough to have worked with <a href="https://ontariotechu.ca/experts/feas/scott-nokleby.php" rel="external nofollow noopener" target="_blank">Dr. Scott Nokleby</a> under the <code class="language-plaintext highlighter-rouge">Canadian MITACS-Globalink'21 Research Scholarship</code>.</p> <p><code class="language-plaintext highlighter-rouge">Note</code> : <code class="language-plaintext highlighter-rouge">I am actively seeking internship opportunities for Summer and Spring 2025.</code></p> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%6A%75%6E%61%72%30%30%32@%75%6D%6E.%65%64%75" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=38XpwpkAAAAJ&amp;hl" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/pranay-junare" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/pranay-junare" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a> <a href="/assets/pdf/PranayJunare_Resume.pdf" title="Curriculum Vitae"><i class="fa fa-file-text"></i></a> </div> <div class="contact-note"> Hi! The best way to reach me is via email (or sometimes a LinkedIn DM). </div> </div> <h2><a href="/news/" style="color: inherit;">News</a></h2> <div class="news"> <div class="table-responsive" style="max-height: 20vh"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Aug 26, 2024</th> <td> Started working as a Research Assistant at FFIL Laboratory working on 3D Scene reconstruction and UAV swarm intelligence. </td> </tr> <tr> <th scope="row">Aug 26, 2024</th> <td> Started my Masters journey at <a href="https://cse.umn.edu/mnri" rel="external nofollow noopener" target="_blank">University of Minnesota - Twin Cities, USA</a> </td> </tr> <tr> <th scope="row">Aug 16, 2024</th> <td> Adios ION Trading! Today marks my last day at this amazing place. </td> </tr> <tr> <th scope="row">Jul 31, 2024</th> <td> Glad to share that I have been awarded JN Tata Scholarship-2024! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">Dec 14, 2023</th> <td> Super excited to meet and connect with multiple roboticists at <a href="https://rosconindia.in/" rel="external nofollow noopener" target="_blank">ROSCon India 2023</a> at IISc, Bangalore. </td> </tr> <tr> <th scope="row">Jul 10, 2023</th> <td> Had fun attending <a href="https://www.iri.upc.edu/workshops/RoboticsAISummerSchool2023/" rel="external nofollow noopener" target="_blank">Robotics &amp; AI Summer School 2023</a> at IRI-UPC, Spain. </td> </tr> <tr> <th scope="row">Dec 1, 2022</th> <td> Presented our final year work at IEEE India Council’s flagship conference - INDICON’22. </td> </tr> <tr> <th scope="row">Jun 1, 2022</th> <td> Won “Best Final Year BTech Project Award” for our Thesis project “Deep Learning based end-to-end Robotic Grasping pipeline on a low-cost 5-DOF robotic arm”. </td> </tr> <tr> <th scope="row">May 1, 2022</th> <td> Won Consolation prize at “DTE’s Project Competition’22” among 110+ teams. </td> </tr> <tr> <th scope="row">May 1, 2022</th> <td> Won 3rd prize at “M-Exhibit UG Project Competition’22” among 40+ teams. </td> </tr> <tr> <th scope="row">Dec 25, 2021</th> <td> Our work on Visual SLAM has been published in IEEE ICCCA’21 Conference. </td> </tr> <tr> <th scope="row">Dec 1, 2021</th> <td> Presented my very first research paper in ARMS at DRDO-Pune! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">May 1, 2021</th> <td> Started internship at prestigious NTU-Singapore under Dr. Xie Ming. Working at the intersection of Robotics. Computer Vision and Deep Learning. </td> </tr> <tr> <th scope="row">May 1, 2021</th> <td> Started my Summer Research Internship at Canada’s OntarioTech University under MITACS-GRI. In addition, received scholarship of $15,000 for future graduate studies. </td> </tr> </table> </div> </div> <p></p> <h2><a href="/publications/" style="color: inherit;">Featured Publications</a></h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3.5 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Grasping_DL.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Grasping_DL.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Grasping_DL.gif-1400.webp"></source> <img src="/assets/img/publication_preview/Grasping_DL.gif" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Grasping_DL.gif" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="junare2022deep" class="col-sm-8"> <div class="title">Deep Learning based end-to-end Grasping Pipeline on a lowcost 5-DOF Robotic arm</div> <div class="author"> <em>Pranay Junare</em>, Mihir Deshmukh, Mihir Kulkarni, and Prashant Bartakke</div> <div class="periodical"> <em>In IEEE 19th India Council International Conference (INDICON)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/document/10040180" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/papers/indicon_grasping.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.youtube.com/watch?v=a12LXmxJKR4" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>The problem of robotic grasping is still an unsolved problem with many approaches trying to generalize grasp predictions for unseen and dynamic environments. In this paper, we propose a complete end-to-end pipeline for the task of Deep Learning based robotic grasping on a lowcost 5-DOF arm. We explore Transfer learning approach and then train our grasping model from end-to-end. In the transfer learning approach we tried 2 base models, VGG-16 and ResNet-50. Our grasping model when ResNet-50 is used as base architecture provided better results with a testing accuracy of 83.3% while VGG-16 provided an accuracy of 78.2%. In order to test our model on a real robotic arm, we built a 5-DOF arm and added a custom parallel plate gripper. Complete ROS and Moveit support is added to our developed robotic arm. The processed RG-D image from the KinectV2 camera is given as an input to the model which predicts the 5-D grasp configuration. Required electronic system design and its PCB is built which controls the robotic arm. The predicted 5-D grasp configuration is then transformed to the object pose w.r.t the base link frame of the robot. Lastly, a ROS node is written that automates the task of picking objects lying in different positions &amp; orientations and sends the joint angle values over pyserial communication to the microcontroller’s PCB.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3.5 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview//RPM-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview//RPM-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview//RPM-1400.webp"></source> <img src="/assets/img/publication_preview//RPM.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="/RPM.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="kulkarni2021visual" class="col-sm-8"> <div class="title">Visual SLAM Combined with Object Detection for Autonomous Indoor Navigation using Kinect V2 and ROS</div> <div class="author"> Mihir Kulkarni, <em>Pranay Junare</em>, Mihir Deshmukh, and Priti P Rege</div> <div class="periodical"> <em>In IEEE 6th International Conference on Computing, Communication and Automation (ICCCA)</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/document/9666426" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/papers/iccca_vslam.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.youtube.com/watch?v=-NRKtv8KcNM" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>SLAM can be defined as exploring the unknown environment while mapping the robot’s surroundings alongside estimating its pose (i.e., position and orientation). It is primarily done using the sensors mounted on the robot. SLAM enables us to autonomously navigate the robot throughout the map based on given final goal coordinates or waypoints. However, SLAM algorithms alone are not capable of performing complex tasks such as autonomous payload delivery in warehouses, healthcare facilities, etc. These tasks require additional semantic information about the environment. To solve this problem, we propose a solution where the traditional Visual SLAM method is accompanied by object detection using pre-trained CNNs to enhance the robot’s capabilities of navigating efficiently and performing robust 3D perception in indoor environments. RTAB-Map using the KinectV2 RGB-D Camera is selected to perform Visual SLAM while the YOLO V3 tiny model acts as the CNN detector for detecting objects of interest. Development platform used is ROS &amp; Gazebo. The proposed solution is experimentally verified by simulating the Turtlebot in the Gazebo environment.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3.5 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Hubot_short.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Hubot_short.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Hubot_short.gif-1400.webp"></source> <img src="/assets/img/publication_preview/Hubot_short.gif" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Hubot_short.gif" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="junaredevelopment" class="col-sm-8"> <div class="title">Development of Robotic Arm Manipulator mounted on Self Balancing Two Wheeled Mobile Robot</div> <div class="author"> <em>Pranay Junare</em>, Shaunak Mahajan, Anirudh Nallawar, and SS Ohol</div> <div class="periodical"> <em>Aerospace and Defence Related Mechanisms Symphosium (ARMS)</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.researchgate.net/publication/358211858_Development_of_Robotic_Arm_Manipulator_mounted_on_Self_Balancing_Two_Wheeled_Mobile_Robo" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/papers/hubot21108Pranay.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.youtube.com/watch?v=CkbDVIfOazs" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>Self-balancing robots are becoming increasingly popular now a days. They have better agility and are compact in size when compared to four-wheeled mobile robots. On the other hand, robotic arms are widely used in manufacturing units and industrial automation processes. But most of these robotic arms lack the navigation capabilities. Thus, to overcome these issues of mobile arm manipulation and performing swift locomotion in narrow dense areas, we have built a robotic system. It has two wheeled self-balancing mobile base and two 5DOF arms driven by servo actuators capable of mimicking the humanoid robot movement. For accurate state estimation of the robot, angles computed by implementation of MEMS sensor fusion algorithms is used. The Cascaded PID Controller is designed to realize the movement of the robot in forward and backward direction. Thus, the complete methodology used to maintain the robot upright throughout the unprecedented disturbance along with mechanical system design and mathematical modeling is mentioned in this paper. Later the simulation of the robotic system is done using Gazebo and ROS and finally verification of the simulation results is done by actual hardware implementation.</p> </div> </div> </div> </li> </ol> </div> <h2><a href="/experience/" style="color: inherit;">Research and Industrial Experience</a></h2> <div class="experience_display"> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Pranay Junare. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-06M6DMH1TB"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-06M6DMH1TB");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>